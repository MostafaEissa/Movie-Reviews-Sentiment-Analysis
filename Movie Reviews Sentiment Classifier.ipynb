{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Classification using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my implementation of using a Naive Bayes classifier to do sentiment analysis on movie review dataset.\n",
    "i.e. given a moview review the target is to output its sentiment whether positive, negative or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model we will use the [Movie Review Dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "It was made available in 2004 by Bo Pang and Lillian Lee. Around 2,000 moview reviews are included in the dataset that are annonated as either `positive` or `negative`. It is about 3.8MB in size and can be\n",
    "downloaded from this [link](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/movie_reviews.zip).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you download the dataset, it contains 2 folders: pos and neg.\n",
    "\n",
    "<img src=\"images/dataset folders.png\" width=\"200px\" />\n",
    "\n",
    "The folder name indicate the true sentiment of the files inside. Each folder contains 1000 text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_text = [] #stored as np array\n",
    "reviews_label = [] #labels (pos or neg)\n",
    "\n",
    "classes = ['pos', 'neg']\n",
    "current_path = os.getcwd()\n",
    "\n",
    "for i in classes:\n",
    "    path = os.path.join(current_path, 'movie_reviews', str(i))\n",
    "    reviews = os.listdir(path)\n",
    "    \n",
    "    for review in reviews:\n",
    "        file = open(path + '\\\\' + review, 'r') \n",
    "        txt = file.read() \n",
    "        reviews_text.append(txt)\n",
    "        reviews_label.append(i)\n",
    "        \n",
    "reviews_text = np.array(reviews_text)\n",
    "reviews_label = np.array(reviews_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of data\n",
    "reviews_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each moview review consists of a bunch of words spread over multiple sentences. \n",
    "Before we proceed further we need to tokenize each review into a list of words. For that we will use the\n",
    "`nltk.tokenize` [package](http://www.nltk.org/api/nltk.tokenize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mostafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = reviews_label\n",
    "for review in reviews_text:\n",
    "    words = word_tokenize(review)\n",
    "    words=np.array([word.lower() for word in words if word.isalpha()])\n",
    "    X.append(words)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is a good idea to split the data into training data and testing data where the former is used in training the model while the later is used to evaluate its performance on unseen data before deploying it into a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, labels, test_size=0.2, random_state=0):\n",
    "    np.random.seed(random_state)\n",
    "    N = labels.shape[0]\n",
    "    idx = np.random.permutation(N)\n",
    "    train_size = int(np.ceil((1-test_size)*N))\n",
    "    X_train = data[idx[:train_size]]\n",
    "    y_train = labels[idx[:train_size]]\n",
    "    X_test = data[idx[train_size:]]\n",
    "    y_test = labels[idx[train_size:]]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1600,)\n",
      "X_test shape: (400,)\n",
      "y_train shape: (1600,)\n",
      "y_test shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=113)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup, we will limit our feature to the most common 2000 words in the corpus. So first, \n",
    "we need to determine the most common words and then convert each input as a 2000-dimensional verctor\n",
    "were each component determine whether the i-th word is present in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all words in the dataset\n",
    "all_words = []\n",
    "for rev in X_train:\n",
    "    all_words.extend(rev.ravel())\n",
    "\n",
    "# sort by frequency\n",
    "all_words = nltk.FreqDist(w for w in all_words)\n",
    "\n",
    "# pick he top 2000 most frequent\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
